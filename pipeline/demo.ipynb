{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out image captioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.image.llava_onevision import LLAVAOneVision\n",
    "from models.image.internvl import InternVL\n",
    "image_captioner = LLAVAOneVision()\n",
    "image_captioner_2 = InternVL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captioner_2.ask_question(\"choose 100 random videos from a directory in Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captioner.caption_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10k images (src/image_captions.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering of words in descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from collections import Counter\n",
    "import re\n",
    "descriptions = pd.read_csv(\"tenkframe_descriptions.csv\")\n",
    "objects = pd.read_csv(\"/ccn2/dataset/babyview/outputs_20250312/yoloe/cdi_10k/bounding_box_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count object occurrences\n",
    "objects_filtered = objects[objects[\"class_name\"] != \"person\"]\n",
    "\n",
    "object_counts = objects_filtered[\"class_name\"].value_counts()\n",
    "\n",
    "object_counts_filtered = object_counts[object_counts > 10]\n",
    "\n",
    "# 2. Count how often each class name is mentioned in descriptions\n",
    "caption_text = \" \".join(descriptions[\"caption\"]).lower()\n",
    "caption_counts = {}\n",
    "for label in object_counts_filtered.index:\n",
    "    pattern = r'\\b' + re.escape(label.lower()) + r's?\\b'  # match singular/plural\n",
    "    caption_counts[label] = len(re.findall(pattern, caption_text))\n",
    "\n",
    "# 3. Combine into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"class_name\": object_counts_filtered.index,\n",
    "    \"object_count\": object_counts_filtered.values,\n",
    "    \"caption_count\": [caption_counts[k] for k in object_counts_filtered.index]\n",
    "})\n",
    "\n",
    "# 4. Scatter plot using plotnine\n",
    "plot = (\n",
    "    ggplot(df, aes(x=\"object_count\", y=\"caption_count\", label=\"class_name\")) +\n",
    "    geom_point(size=4, color=\"steelblue\") +\n",
    "    geom_smooth(method=\"lm\") +\n",
    "    geom_text(nudge_y=0.2, size=10, ha='left') +\n",
    "    labs(\n",
    "        title=\"Objects vs. Mentions in Captions\",\n",
    "        x=\"Count in objects\",\n",
    "        y=\"Mentions in captions\"\n",
    "    )  + theme(\n",
    "        figure_size=(20, 12),  # Increase plot size\n",
    "        axis_text_x=element_text(rotation=45, hjust=1)\n",
    "    )\n",
    ")\n",
    "\n",
    "plot\n",
    "\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "correlation, p_value = pearsonr(df[\"object_count\"], df[\"caption_count\"])\n",
    "print(f\"Pearson correlation: {correlation:.2f}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing images to objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing descriptions to objects mentioned --\n",
    "df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 minute long chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_utils import split_video\n",
    "vid_path = \"/ccn2/dataset/babyview/unzip_2025/babyview_main_storage/00220001_2024-05-31_1_acd11db79d/00220001_2024-05-31_1_acd11db79d_processed.MP4\"\n",
    "split_video(vid_path, 60, \"/ccn2/dataset/babyview/outputs_20250312/activities/chunks/00220001_2024-05-31_1_acd11db79d\", keep_audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out VideoChat_Flash and LLava video \"describe this video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from models.video.videochat_flash import VideoFlash\n",
    "video_generator = VideoFlash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these four activity types, which do you think these videos fall under? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinetics 400 but for kids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/ccn2/dataset/babyview/outputs_20250312/activities/chunks/00820001_2024-04-09_2_4aa5f86d25_processed\"\n",
    "activities = [\"being held\", \"eating\", \"drinking\", \"playing with toy\", \"getting changed\", \"crawling\", \"crying\", \"exploring\", \"cooking\", \"cleaning\", \"gardening\", \"watching tv\", \"driving\", \"reading\"]\n",
    "activity, _ = video_generator.caption_video(f\"{output_dir}/chunk003.mp4\",f\"Answer with one word what activity is going on in this video, taken with a camera attached to the head of a child, from the following options: {\", \".join(activities)}\")\n",
    "print(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these four areas, which do you think these videos are in? Dining room, living room, outside, bedroom. Can we extend Lew-Williams 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption, _ = video_generator.caption_video(df[\"chunk_path\"][0], question=\"Of dining room, living room, bedroom, kitchen and outside, which of these do you think this video takes place in?\")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing areas in the two videos to objects detected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 minute long video -- maybe try out InternVL2.5_HiCo_R64 and see if we can get timestamp information. also look at descriptions from the other internvideo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption, _ = video_generator.caption_video()\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 1000 random video chunks and testing out activity captioning with them. To do this we're using 100 random videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 video paths to random_video_paths.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory\n",
    "base_dir = '/ccn2/dataset/babyview/unzip_2025/babyview_main_storage'\n",
    "\n",
    "# List all immediate subdirectories\n",
    "subdirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir)\n",
    "           if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# Randomly sample up to 100 subdirectories\n",
    "sampled_subdirs = random.sample(subdirs, min(100, len(subdirs)))\n",
    "\n",
    "# For each sampled subdir, find the first .mp4 file inside\n",
    "sampled_video_paths = []\n",
    "for subdir in sampled_subdirs:\n",
    "    for file in os.listdir(subdir):\n",
    "        if file.lower().endswith('.mp4'):\n",
    "            sampled_video_paths.append(os.path.join(subdir, file))\n",
    "            break  # Assume one MP4 per subdir\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = 'random_video_paths.csv'\n",
    "df = pd.DataFrame({'video_path': sampled_video_paths})\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Saved {len(sampled_video_paths)} video paths to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5566"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subdirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok just getting all of the video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5566 video paths to all_video_paths.csv\n"
     ]
    }
   ],
   "source": [
    "# For each sampled subdir, find the first .mp4 file inside\n",
    "all_video_paths = []\n",
    "for subdir in subdirs:\n",
    "    for file in os.listdir(subdir):\n",
    "        if file.lower().endswith('.mp4'):\n",
    "            all_video_paths.append(os.path.join(subdir, file))\n",
    "            break  # Assume one MP4 per subdir\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = 'all_video_paths.csv'\n",
    "df = pd.DataFrame({'video_path': all_video_paths})\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Saved {len(all_video_paths)} video paths to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from video_utils import split_video\n",
    "df = pd.read_csv(\"random_video_paths.csv\")\n",
    "sampled_video_paths = df[\"video_path\"]\n",
    "for vid_path in tqdm(sampled_video_paths, desc=\"Chunking videos\"):\n",
    "    curr_video_id = Path(vid_path).stem\n",
    "    split_video(vid_path, 60, f\"/ccn2/dataset/babyview/outputs_20250312/activities/chunks/{curr_video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"bathroom\", \"bedroom\", \"car\", \"closet\", \"garage\", \"living room\", \"hallway\", \"outside\", \"garage\", \"kitchen\", \"deck\"]\n",
    "activities = [\"being held\", \"eating\", \"drinking\", \"playing with toy\", \"getting changed\", \"crawling\", \"crying\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "descriptions = pd.read_csv(\"tenkframe_descriptions.csv\")\n",
    "df = []\n",
    "for image_path, caption in tqdm(zip(descriptions[\"image_path\"], descriptions[\"caption\"]), desc=\"Getting image descriptions\"):\n",
    "    location = image_captioner_2.caption_image(image_path,f\"Answer with one word what location this image is in from the following: {\", \".join(locations)}\")\n",
    "    activity = image_captioner_2.caption_image(image_path,f\"Answer with one word what activity is going on in this image in from the following: {\", \".join(activities)}\")\n",
    "    df.append({\n",
    "        image_path: image_path,\n",
    "        location: location,\n",
    "        activity: activity,\n",
    "        caption: caption\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions[\"image_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "def render_image(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "render_image(descriptions[\"image_path\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for video chunking. Let's see if we can pull transcripts too that would be neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from video_utils import split_video_simple\n",
    "df = pd.read_csv(\"selected_chunk_transcripts.csv\")\n",
    "sampled_video_paths = df[\"video_path\"]\n",
    "for vid_path in tqdm(sampled_video_paths, desc=\"Chunking videos\"):\n",
    "    curr_video_id = Path(vid_path).stem\n",
    "    split_video_simple(vid_path, 60, f\"/ccn2/dataset/babyview/outputs_20250312/activities/chunks/{curr_video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.video.llava_video import LLAVAVideo\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"selected_chunk_transcripts.csv\")\n",
    "video_generator2 = LLAVAVideo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"chunk_path\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting location frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"image_activities_locations_10k.csv\")\n",
    "df['location_clean'] = df['location'].str.lower()\n",
    "keywords = [\"bathroom\", \"bedroom\", \"car\", \"closet\", \"garage\", \"living room\", \"hallway\", \"outside\", \"garage\", \"kitchen\", \"deck\"]\n",
    "\n",
    "# Find the first matching keyword\n",
    "def first_match(text):\n",
    "    for word in keywords:\n",
    "        if word in text:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "df['matched'] = df['location_clean'].apply(first_match)\n",
    "\n",
    "# Drop unmatched\n",
    "df = df.dropna(subset=['matched'])\n",
    "\n",
    "# Plot\n",
    "plot = (\n",
    "    ggplot(df, aes(x='matched')) +\n",
    "    geom_bar() +\n",
    "    labs(\n",
    "        title='Location counts',\n",
    "        x='Location',\n",
    "        y='Count'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"image_activities_locations_10k.csv\")\n",
    "df['location_clean_p1'] = df['location'].str.lower()\n",
    "keywords = [\"bathroom\", \"bedroom\", \"car\", \"closet\", \"garage\", \"living room\", \"hallway\", \"outside\", \"garage\", \"kitchen\", \"deck\"]\n",
    "\n",
    "# Find the first matching keyword\n",
    "def first_match(text):\n",
    "    for word in keywords:\n",
    "        if word in text:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "df['location_clean'] = df['location_clean_p1'].apply(first_match)\n",
    "\n",
    "# Drop unmatched\n",
    "df = df.dropna(subset=['location_clean'])\n",
    "df = df.drop(columns=[\"location\", \"location_clean_p1\"])\n",
    "df.to_csv(\"image_locations_clean.csv\", index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "df = pd.read_csv(\"video_activities_locations_all.csv\")\n",
    "df['activity_clean'] = df['activity_transcript'].str.lower()\n",
    "keywords = [\"being held\", \"eating\", \"drinking\", \"playing with toy\", \"getting changed\", \"crawling\", \"crying\", \"exploring\", \"cooking\", \"cleaning\", \"gardening\", \"watching tv\", \"driving\", \"reading\", \"on a phone call\", \"dancing\", \"packing\", \"looking at phone\", \"instrument playing\", \"exercising\", \"working on laptop\", \"nothing\"]\n",
    "\n",
    "# Find the first matching keyword\n",
    "def first_match(text):\n",
    "    for word in keywords:\n",
    "        if word in text:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "df['matched'] = df['activity_clean'].apply(first_match)\n",
    "\n",
    "# Drop unmatched\n",
    "df = df.dropna(subset=['matched'])\n",
    "\n",
    "# Plot\n",
    "plot = (\n",
    "    ggplot(df, aes(x='matched')) +\n",
    "    geom_bar() +\n",
    "    labs(\n",
    "        title='Activity counts',\n",
    "        x='Activity',\n",
    "        y='Count'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [\"being held\", \"eating\", \"drinking\", \"playing with toy\", \"getting changed\", \"crawling\", \"crying\", \"exploring\", \"cooking\", \"cleaning\", \"gardening\", \"watching tv\", \"driving\", \"reading\", \"on a phone call\", \"dancing\", \"packing\", \"looking at phone\", \"instrument playing\", \"exercising\", \"working on laptop\"]\n",
    "activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activitycap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
