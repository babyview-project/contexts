{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from vendi_score import vendi\n",
    "from pprint import pprint   \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1094223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Set this to either images or videos\n",
    "images_or_videos = 'images'  # 'images' or 'videos'\n",
    "\n",
    "if images_or_videos == 'videos':\n",
    "    embedding_model = 'facebook_vjepa2-vitl-fpc64-256'\n",
    "    embedding_dir_dict = {\n",
    "        'BabyView': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/babyview/{embedding_model}',\n",
    "        'SAYCam': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/SAYCam/{embedding_model}',\n",
    "        'Ego4D': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/ego4D/{embedding_model}',\n",
    "        'Kinetics400': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/kinetics_train//{embedding_model}',\n",
    "        'SSv2': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/ssv2/{embedding_model}',\n",
    "        'MomentsInTime': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/Moments_in_Time_Raw_training/{embedding_model}',\n",
    "        'Physion': f'/ccn2a/dataset/babyview/2025.2/outputs/video_embeddings/physion/{embedding_model}',\n",
    "    }\n",
    "    \n",
    "elif images_or_videos == 'images':\n",
    "    embedding_model = 'facebook_dinov3-vitb16-pretrain-lvd1689m'\n",
    "    # embedding_model = 'facebook_dinov2-base'\n",
    "    embedding_dir_dict = {\n",
    "        'BabyView': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/babyview/{embedding_model}',\n",
    "        'SAYCam': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/SAYCam/{embedding_model}',\n",
    "        'Ego4D': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/ego4D/{embedding_model}',\n",
    "        'Kinetics400': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/kinetics400_train//{embedding_model}',\n",
    "        'SSv2': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/ssv2/{embedding_model}',\n",
    "        'MomentsInTime': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/Moments_in_Time_Raw_training/{embedding_model}',\n",
    "        'Physion': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/physion/{embedding_model}',\n",
    "        'ImageNet': f'/ccn2a/dataset/babyview/2025.2/outputs/image_embeddings/imagenet_test/{embedding_model}',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_files(embedding_dir):\n",
    "    files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "    return len(files)\n",
    "\n",
    "def load_embeddings(embedding_dir, num_samples=1000):\n",
    "    files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "    files = np.random.choice(files, min(num_samples, len(files)), replace=False)\n",
    "    X = np.stack([np.load(os.path.join(embedding_dir, f)) for f in files])  # (n, d)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "embedding_dim = None\n",
    "for dataset_name, embedding_dir in embedding_dir_dict.items():\n",
    "    X = load_embeddings(embedding_dir)          # (n, 1024)\n",
    "    embedding_dim = X.shape[1]\n",
    "    num_files = get_num_files(embedding_dir)\n",
    "\n",
    "    results[dataset_name] = {\n",
    "        'num_files': num_files,\n",
    "        'embeddings': X,\n",
    "    }\n",
    "    print(f'{dataset_name}: {num_files} files, {X.shape[0]} samples, {X.shape[1]} dim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c55e81",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfed9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- t-SNE across datasets (continue from your code above) --------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Config: cap how many points per dataset for speed (set to None for all)\n",
    "max_per_dataset = 1000\n",
    "random_state = 42\n",
    "\n",
    "# 1) Collect a balanced sample across datasets\n",
    "Xs = []\n",
    "labels = []\n",
    "row_ids = []  # index within each dataset's embedding array\n",
    "for ds_name, info in results.items():\n",
    "    X = info[\"embeddings\"]  # (n, d)\n",
    "    n = X.shape[0]\n",
    "    if n == 0:\n",
    "        continue\n",
    "    if (max_per_dataset is None) or (n <= max_per_dataset):\n",
    "        idx = np.arange(n)\n",
    "    else:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        idx = rng.choice(n, size=max_per_dataset, replace=False)\n",
    "    Xs.append(X[idx])\n",
    "    labels.extend([ds_name] * len(idx))\n",
    "    row_ids.extend(idx.tolist())\n",
    "\n",
    "X_all = np.vstack(Xs)                      # (N, d)\n",
    "labels = np.array(labels)                  # (N,)\n",
    "row_ids = np.array(row_ids)                # (N,)\n",
    "\n",
    "N, D = X_all.shape\n",
    "print(f\"[t-SNE] Using {N} points across {len(np.unique(labels))} datasets; dim={D}\")\n",
    "\n",
    "# 2) Optional PCA to 50D (common speedup + denoising before t-SNE)\n",
    "pca_dim = min(50, D)\n",
    "X_pca = PCA(n_components=pca_dim, random_state=random_state).fit_transform(X_all)\n",
    "\n",
    "# 3) Pick a valid perplexity based on N\n",
    "#    t-SNE requires (3 * perplexity + 1) < N; keep it in a sane range\n",
    "max_perp = max(5, int((N - 1) / 3))\n",
    "perplexity = min(50, max_perp)  # cap at 50 by default\n",
    "print(f\"[t-SNE] Using perplexity={perplexity}\")\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perplexity,\n",
    "    learning_rate=200,   # use numeric for compatibility\n",
    "    init=\"pca\",\n",
    "    random_state=random_state,\n",
    "    verbose=1,\n",
    ")\n",
    "X_2d = tsne.fit_transform(X_pca)\n",
    "\n",
    "\n",
    "# 4) Plot â€” honor embedding_dir_dict order for colors & legend\n",
    "plt.figure(figsize=(8, 7), dpi=120)\n",
    "\n",
    "# Datasets in the exact insertion order of embedding_dir_dict,\n",
    "# but keep only those present in the current labels\n",
    "ordered_datasets_all = list(embedding_dir_dict.keys())\n",
    "present = set(labels.tolist())\n",
    "ordered_datasets_present = [ds for ds in ordered_datasets_all if ds in present]\n",
    "\n",
    "# Stable color map in that exact order\n",
    "num_classes = len(ordered_datasets_present)\n",
    "cmap = plt.get_cmap(\"tab20\" if num_classes > 10 else \"tab10\")\n",
    "color_map = {ds: cmap(i % cmap.N) for i, ds in enumerate(ordered_datasets_present)}\n",
    "\n",
    "# Reproducible RNG for plotting subsample\n",
    "rng = np.random.default_rng(random_state)\n",
    "\n",
    "handles = []\n",
    "for ds in ordered_datasets_present:\n",
    "    mask = (labels == ds)\n",
    "    ds_idx = np.flatnonzero(mask)\n",
    "    # Plot at most 200 points for this dataset\n",
    "    if ds_idx.size > 200:\n",
    "        plot_idx = rng.choice(ds_idx, size=200, replace=False)\n",
    "    else:\n",
    "        plot_idx = ds_idx\n",
    "\n",
    "    sc = plt.scatter(\n",
    "        X_2d[plot_idx, 0],\n",
    "        X_2d[plot_idx, 1],\n",
    "        s=10,\n",
    "        alpha=0.75,\n",
    "        c=[color_map[ds]],\n",
    "        label=f\"{ds} (n={plot_idx.size})\",\n",
    "        edgecolors=\"none\",\n",
    "    )\n",
    "    handles.append(sc)\n",
    "\n",
    "plt.title(f\"t-SNE of {images_or_videos} using {embedding_model}\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "# Legend order matches embedding_dir_dict because we pass handles in order\n",
    "plt.legend(handles=handles, loc=\"best\", frameon=True, fontsize=8, markerscale=2.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 5) Save outputs\n",
    "out_png = f\"figures/tsne_{images_or_videos}.png\"\n",
    "os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "plt.savefig(out_png)\n",
    "print(f\"[t-SNE] Figure saved to: {out_png}\")\n",
    "\n",
    "df_out = pd.DataFrame({\n",
    "    \"x\": X_2d[:, 0],\n",
    "    \"y\": X_2d[:, 1],\n",
    "    \"dataset\": labels,\n",
    "    \"row_id_within_dataset\": row_ids,\n",
    "})\n",
    "# If running in a notebook, also show it inline:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87b1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1b737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
