```{r}
library(tidyverse)
library(here)
library(glue)
library(tidytext)
library(irr)
library(psych)
```

```{r}
ann_1 <- read.csv(here("data/completed_annotations/DD_list_final.csv"))
ann_2 <- read.csv(here("data/completed_annotations/JY_list_final.csv"))
ann_3 <- read.csv(here("data/completed_annotations/VM_list_final.csv"))
df_cleaned <- read.csv(here("data/all_contexts_cleaned.csv"))
```

# main IRR and precision calculations across raters
```{r}
# Step 1: Add annotator ID and combine all annotation files (including Other columns)
ann_1_labeled <- ann_1 %>%
  mutate(annotator = "DD") %>%
  select(video_id, Location, Activity, Other.locations, Other.activities, 
         Video.description.accuracy..1.5., annotator)

ann_2_labeled <- ann_2 %>%
  mutate(annotator = "JY") %>%
  select(video_id, Location, Activity, Other.locations, Other.activities, 
         Video.description.accuracy..1.5., annotator)

ann_3_labeled <- ann_3 %>%
  mutate(annotator = "VM") %>%
  select(video_id, Location, Activity, Other.locations, Other.activities, 
         Video.description.accuracy..1.5., annotator)

# Combine all annotations
all_annotations <- bind_rows(ann_1_labeled, ann_2_labeled, ann_3_labeled)

# Step 2: Filter for rows where Video.description.accuracy is not empty for all three annotators
complete_ratings <- all_annotations %>%
  filter(!is.na(Video.description.accuracy..1.5.) & 
         Video.description.accuracy..1.5. != "" &
         !is.null(Video.description.accuracy..1.5.)) %>%
  group_by(video_id) %>%
  filter(n() == 3) %>%  # Only keep video_ids that have all 3 annotations
  ungroup()

# Get the video_ids that have complete ratings
complete_video_ids <- unique(complete_ratings$video_id)
clean_video_ids <- sub("^\\d+_", "", complete_video_ids)

print(paste("Number of videos with complete ratings from all three annotators:", length(clean_video_ids)))

# Step 3: Find matching rows in df_cleaned
df_matched <- df_cleaned %>%
  filter(video_id %in% clean_video_ids)

print(paste("Number of rows in df_cleaned matching complete annotations:", nrow(df_matched)))

# Step 4: Prepare data for inter-rater reliability analysis
# Pivot wider to have one row per video_id with columns for each annotator
location_wide <- complete_ratings %>%
  select(video_id, annotator, Location) %>%
  pivot_wider(names_from = annotator, values_from = Location, names_prefix = "Location_")

activity_wide <- complete_ratings %>%
  select(video_id, annotator, Activity) %>%
  pivot_wider(names_from = annotator, values_from = Activity, names_prefix = "Activity_")

# Video description accuracy ratings
accuracy_wide <- complete_ratings %>%
  select(video_id, annotator, Video.description.accuracy..1.5.) %>%
  mutate(Video.description.accuracy..1.5. = as.numeric(Video.description.accuracy..1.5.)) %>%
  pivot_wider(names_from = annotator, values_from = Video.description.accuracy..1.5., 
              names_prefix = "Accuracy_")

print("=== INTER-RATER RELIABILITY ANALYSIS ===")

# For Location
if(all(c("Location_DD", "Location_JY", "Location_VM") %in% colnames(location_wide))) {
  location_matrix <- location_wide %>%
    select(Location_DD, Location_JY, Location_VM) %>%
    as.matrix()
  
  # Calculate Fleiss' Kappa for Location 
  fleiss_kappa <- kappam.fleiss(location_matrix)
  kripp_alpha <- kripp.alpha(t(location_matrix), method="nominal")
  print(paste("Location - Fleiss Kappa:", round(fleiss_kappa$value, 3)))
  # Calculate agreement percentages
  location_agreement_DD_JY <- mean(location_matrix[,1] == location_matrix[,2], na.rm = TRUE)
  location_agreement_DD_VM <- mean(location_matrix[,1] == location_matrix[,3], na.rm = TRUE)
  location_agreement_JY_VM <- mean(location_matrix[,2] == location_matrix[,3], na.rm = TRUE)
  print(paste("Location Agreement DD-JY:", round(location_agreement_DD_JY * 100, 1), "%"))
  print(paste("Location Agreement DD-VM:", round(location_agreement_DD_VM * 100, 1), "%"))
  print(paste("Location Agreement JY-VM:", round(location_agreement_JY_VM * 100, 1), "%"))
  print(paste("Location Kripp alpha:", round(kripp_alpha$value, 3)))
}

# For Activity
if(all(c("Activity_DD", "Activity_JY", "Activity_VM") %in% colnames(activity_wide))) {
  activity_matrix <- activity_wide %>%
    select(Activity_DD, Activity_JY, Activity_VM) %>%
    as.matrix()
  
  # Calculate agreement percentages
      activity_kappa <- kappa2(activity_matrix[,1:2])
      fleiss_kappa <- kappam.fleiss(activity_matrix)
      kripp_alpha <- kripp.alpha(t(activity_matrix), method="nominal")
  print(paste("Activity - Cohen's Kappa (DD vs JY):", round(activity_kappa$value, 3)))
  activity_kappa_2 <- kappa2(activity_matrix[,c(1,3)])
  print(paste("Activity - Cohen's Kappa (DD vs VM):", round(activity_kappa_2$value, 3)))
  activity_kappa_3 <- kappa2(activity_matrix[,2:3])
  print(paste("Activity - Cohen's Kappa (VM vs JY):", round(activity_kappa_3$value, 3)))
  print(paste("Activity - Fleiss Kappa:", round(fleiss_kappa$value, 3)))
  print(paste("Activity - Kripp Alpha:", round(kripp_alpha$value, 3)))
}

print("\n=== VIDEO DESCRIPTION ACCURACY RATINGS ===")

if(all(c("Accuracy_DD", "Accuracy_JY", "Accuracy_VM") %in% colnames(accuracy_wide))) {
  accuracy_matrix <- accuracy_wide %>%
    select(Accuracy_DD, Accuracy_JY, Accuracy_VM) %>%
    as.matrix()

  # Calculate means and standard deviations for each annotator
  for(annotator in c("DD", "JY", "VM")) {
    col_name <- paste0("Accuracy_", annotator)
    ratings <- accuracy_wide[[col_name]]
    mean_rating <- mean(ratings, na.rm = TRUE)
    sd_rating <- sd(ratings, na.rm = TRUE)
    print(paste(paste0("Annotator ", annotator, " - Mean:"), round(mean_rating, 2), 
                "SD:", round(sd_rating, 2)))
  }
  
  # Overall statistics across all ratings
  all_ratings <- c(accuracy_matrix)
  all_ratings <- all_ratings[!is.na(all_ratings)]
  print(paste("Overall Mean Rating:", round(mean(all_ratings), 2)))
  print(paste("Overall SD Rating:", round(sd(all_ratings), 2)))
}

# Step 6: Functions for inter-rater reliability calculations

# Function to calculate 3-way agreement percentage
calculate_three_way_agreement <- function(col1, col2, col3) {
  # All three agree
  all_three_agree <- mean(col1 == col2 & col2 == col3, na.rm = TRUE)
  # At least two agree (majority)
  at_least_two_agree <- mean(
    (col1 == col2) | (col1 == col3) | (col2 == col3), 
    na.rm = TRUE
  )
  return(list(all_three = all_three_agree, at_least_two = at_least_two_agree))
}


# Function to calculate set-based agreement (for expanded annotations)
calculate_set_agreement <- function(set1_list, set2_list) {
  if(length(set1_list) != length(set2_list)) {
    return(NA)
  }
  
  agreements <- numeric(length(set1_list))
  
  for(i in seq_along(set1_list)) {
    set1 <- set1_list[[i]]
    set2 <- set2_list[[i]]
    
    # Check if there's any overlap between the sets
    if(length(intersect(set1, set2)) > 0) {
      agreements[i] <- 1
    } else {
      agreements[i] <- 0
    }
  }
  
  return(mean(agreements, na.rm = TRUE))
}

# Function to calculate 3-way set agreement
calculate_three_way_set_agreement <- function(set1_list, set2_list, set3_list) {
  if(length(set1_list) != length(set2_list) || length(set2_list) != length(set3_list)) {
    return(list(all_three = NA, at_least_two = NA))
  }
  
  all_three_agree <- numeric(length(set1_list))
  at_least_two_agree <- numeric(length(set1_list))
  
  for(i in seq_along(set1_list)) {
    set1 <- set1_list[[i]]
    set2 <- set2_list[[i]]
    set3 <- set3_list[[i]]
    
    # Check pairwise overlaps
    overlap_12 <- length(intersect(set1, set2)) > 0
    overlap_13 <- length(intersect(set1, set3)) > 0
    overlap_23 <- length(intersect(set2, set3)) > 0
    
    # All three have some common element
    common_all <- length(intersect(intersect(set1, set2), set3)) > 0
    all_three_agree[i] <- as.numeric(common_all)
    
    # At least two pairs have overlap
    at_least_two_agree[i] <- as.numeric(overlap_12 | overlap_13 | overlap_23)
  }
  
  return(list(
    all_three = mean(all_three_agree, na.rm = TRUE),
    at_least_two = mean(at_least_two_agree, na.rm = TRUE)
  ))
}

# Function to expand annotations with "Other" columns
expand_annotations <- function(primary_col, other_col) {
  # Handle cases where other_col might be NA or empty
  other_col <- ifelse(is.na(other_col) | other_col == "", "", other_col)
  
  # Split other_col by comma and combine with primary
  other_items <- str_split(other_col, ",\\s*") |>
      lapply(str_trim)
  
  # Create expanded list
  expanded <- map2(primary_col, other_items, function(primary, others) {
    if(length(others) == 1 && others == "") {
      return(primary)
    } else {
      return(c(primary, others))
    }
  })
  
  return(expanded)
}

# Step 8: Calculate F-scores comparing annotations with df_cleaned
print("\n=== F-SCORE ANALYSIS ===")

# Function to calculate F-score for sets (allowing multiple correct answers)
calculate_f_score_sets <- function(human_list, predicted) {
  if(length(human_list) == 0 || length(predicted) == 0) {
    return(list(precision = NA, recall = NA, f1 = NA))
  }
  
  # Convert actual to character and trim
  actual <- str_trim(as.character(predicted))
  
  # Remove NA values
  valid_indices <- !is.na(predicted)
  human_list <- human_list[valid_indices]
  predicted <- predicted[valid_indices]
  
  if(length(human_list) == 0 || length(predicted) == 0) {
    return(list(precision = NA, recall = NA, f1 = NA))
  }
  
  # Trim predicted sets
  human_list <- lapply(human_list, function(x) str_trim(as.character(x)))
  # Calculate population-level metrics
  total_correct <- 0
  total_predicted <- length(predicted)  # Each instance has exactly one true label
  total_actual <- 0
  for(i in seq_along(human_list)) {
    human_set <- human_list[[i]]
    predicted_label <- predicted[i]
    
    if(length(human_set) > 0) {
      total_actual <- total_actual + length(human_set)
      # Count how many predictions in this set are correct
      total_correct <- total_correct + ifelse(predicted_label %in% human_list, 1, 0)
    }
  }

  # Population-level precision and recall
  precision <- ifelse(total_predicted == 0, 0, total_correct / total_predicted)
  recall <- ifelse(total_actual == 0, 0, total_correct / total_actual)
  f1 <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
  
  return(list(precision = precision, recall = recall, f1 = f1))
}

# Corrected standard F-score function (population-level calculation)

calculate_f_score <- function(predicted, actual) {
  # Convert to character and trim strings
  predicted <- str_trim(as.character(predicted))
  actual <- str_trim(as.character(actual))
  
  # Remove NA values
  valid_indices <- !is.na(predicted) & !is.na(actual)
  predicted <- predicted[valid_indices]
  actual <- actual[valid_indices]
  
  if(length(predicted) == 0 || length(actual) == 0) {
    return(list(precision = NA, recall = NA, f1 = NA))
  }
  
  # Get unique labels
  all_labels <- unique(c(predicted, actual))
  
  # Calculate population-level metrics across all labels
  total_tp <- 0
  total_fp <- 0
  total_fn <- 0
  
  for(label in all_labels) {
    tp <- sum(predicted == label & actual == label)
    fp <- sum(predicted == label & actual != label)
    fn <- sum(predicted != label & actual == label)
    
    total_tp <- total_tp + tp
    total_fp <- total_fp + fp
    total_fn <- total_fn + fn
  }
  
  # Population-level precision, recall, and F1
  precision <- ifelse(total_tp + total_fp == 0, 0, total_tp / (total_tp + total_fp))
  recall <- ifelse(total_tp + total_fn == 0, 0, total_tp / (total_tp + total_fn))
  f1 <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
  print(paste("Accuracy", total_tp / 80))
  
  return(list(precision = precision, recall = recall, f1 = f1))
}

# Merge annotations with df_cleaned for comparison
comparison_data <- complete_ratings %>%
  mutate(cleaned_video_id = sub("^\\d+_", "", video_id)) %>%
  left_join(df_cleaned %>% select(video_id, Location, Activity), 
            by = c("cleaned_video_id" = "video_id"), suffix = c("_annotation", "_cleaned"))

# Create expanded annotations including "Other" columns
comparison_data_expanded <- comparison_data %>% 
  mutate(
    expanded_locations = expand_annotations(Location_annotation, Other.locations),
    expanded_activities = expand_annotations(Activity_annotation, Other.activities)
  )


# Calculate F-scores for each annotator vs df_cleaned
annotators <- c("DD", "JY", "VM")

print("--- PRIMARY ANNOTATIONS ONLY ---")
for(annotator in annotators) {
  annotator_data <- comparison_data %>% filter(annotator == !!annotator)
  
  print(paste("\n--- Annotator:", annotator, "---"))
  
  # Location F-score
  location_f_score <- calculate_f_score(annotator_data$Location_annotation, 
                                      annotator_data$Location_cleaned)
  print(paste("Location F1-Score:", round(location_f_score$f1, 3)))
  print(paste("Location Precision:", round(location_f_score$precision, 3)))
  print(paste("Location Recall:", round(location_f_score$recall, 3)))
  
  # Activity F-score
  activity_f_score <- calculate_f_score(annotator_data$Activity_annotation, 
                                       annotator_data$Activity_cleaned)
  print(paste("Activity F1-Score:", round(activity_f_score$f1, 3)))
  print(paste("Activity Precision:", round(activity_f_score$precision, 3)))
  print(paste("Activity Recall:", round(activity_f_score$recall, 3)))
}
```
# expanded contexts within rater
```{r}
activities_expanded <- comparison_data_expanded |> 
  rename(Location_model = Location_cleaned,
         Activity_model = Activity_cleaned) |>
  rowwise() |>
  filter(Activity_model %in% expanded_activities) |>
  group_by(annotator) |>
  count() |>
  mutate(mean = n / 100)

locations_expanded <- comparison_data_expanded |> 
  rename(Location_model = Location_cleaned,
         Activity_model = Activity_cleaned) |>
  rowwise() |>
  filter(Location_model %in% expanded_locations) |>
  group_by(annotator) |>
  count() |> 
  mutate(mean = n / 100)
  
paste("Mean activity precision (when including other activities):", round(mean(activities_expanded$mean), 3))
paste("Mean locatin precision (when including other location):", round(mean(locations_expanded$mean), 3))
saveRDS(comparison_data_expanded, here("data/completed_annotations/comparison_annotation_data.Rds"))
```

# expanded contexts across raters
```{r}
comparison_data_expanded |>
  mutate(Location_model = str_trim(Location_cleaned),
         Activity_model = str_trim(Activity_cleaned)) |>
  rowwise() |>
  mutate(correct_activity_annotation = Activity_model %in% expanded_activities,
         correct_location_annotation = Location_model %in% expanded_locations) |>
  group_by(video_id) |>
  summarize(activity_correct = any(correct_activity_annotation),
            location_correct = any(correct_location_annotation)) |>
  ungroup() |>
  summarize(loc_acc = sum(location_correct==TRUE)/100,
            act_acc = sum(activity_correct==TRUE)/100)

print("Mean number of activities")
mean(sapply(comparison_data_expanded$expanded_activities, length))
sd(sapply(comparison_data_expanded$expanded_activities, length))
print("Mean number of locations")
mean(sapply(comparison_data_expanded$expanded_locations, length))
sd(sapply(comparison_data_expanded$expanded_locations, length))

```
# descriptions
```{r}
paste("Mean video description accuracy:", round(mean(comparison_data_expanded$Video.description.accuracy..1.5.), 2),
      "SD video description accuracy:", round(sd(comparison_data_expanded$Video.description.accuracy..1.5.), 2))
ratings <- accuracy_wide[, c("Accuracy_DD", "Accuracy_JY", "Accuracy_VM")]

kripp_ratings <- irr::kripp.alpha(t(ratings), method = "ordinal")
paste("IRR for video descriptions:", round(kripp_ratings$value, 2))

agreement <- ratings |>
  filter(Accuracy_DD == Accuracy_JY & Accuracy_JY == Accuracy_VM) |>
  count()
paste("Number of frames for which video description accuracy rating matches across all raters:", agreement$n)
```